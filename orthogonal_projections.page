---
title: Orthogonal Projections
categories: Linear Algebra
icon: /img/icons/angles.png
...

<section class="primary">

# Summary

<div class="block">

![Orthogonal Projection in $\bbr^3$](/img/diagrams/linear/orthogonal-projection.png)

The vector $\vec{x}^{\parallel}$ in the decomposition of $\vec{x}$ in $V$:

$$ \vec{x} = \vec{x}^{\perp} + \vec{x}^{\parallel}, $$

is called the **orthogonal projection** of $\vec{x}$ onto $V$, and is denoted by $\text{proj}_{V}(\vec{x})$.
Furthermore, this representation always exists and is unique.

</div>

<div class="proof">

- *Existence*:  
  Consider an orthonormal basis $\inflate{\vec{u}}{m}$ of $V$. If a decomposition $\vec{x} = 
  \vec{x}^{\parallel} + \vec{x}^{\perp}$ does exist, then we can write
  $$ \vec{x}^{\parallel} = \sum_{i=1}^m c_i\vec{u}_i \text{,} $$
  for some coefficients $\inflate{c}{m}$ yet to be determined (since $\vec{x}^{\parallel}$ is in $V$). We know that
  $$ \vec{x}^{\perp} = \vec{x}-\vec{x}^{\parallel} = \vec{x}-\sum_{i=1}c_i\vec{u}_i $$
  is orthogonal to $V$, meaning that $\vec{x}-\sum_{i=1}c_i\vec{u}_i$ is orthogonal to all the vectors $\vec{u}_j$ in $V$:
  $$\begin{align*}
    0 &= \vec{u}_j \cdot \left(\vec{x} - \sum_{i=1}^{m}c_i\vec{u}_i\right) \\
      &= \vec{u}_j\cdot x - \sum_{i=1}^m c_i(\vec{u}_j \cdot \vec{u}_i) = \vec{u}_j\cdot\vec{x} - c_j.
  \end{align*}$$
  It follows that $c_j = \vec{u}_j\cdot\vec{x}$, so that
  $$
    \vec{x}^{\parallel} = (\vec{u}_1\cdot\vec{x})\vec{u}_1 + \cdots
                        + (\vec{u}_j\cdot\vec{x})\vec{u}_j + \cdots
                        + (\vec{u}_m\cdot\vec{x})\vec{u}_m\text{.}
  $$
  Note that $\vec{u}_j\cdot\vec{x}^{\perp} = 0$, by construction, so that $\vec{x}^{\perp}$ is orthogonal
  to $V$, as required.

- *Uniqueness*:  
  Suppose that $\vec{w}$ and $\vec{w}'$ are vectors in $V$ such that $\vec{x} - \vec{w}$ and $\vec{x} - \vec{w}'$ are in 
  $V^{\perp}$. We show $\vec{w} = \vec{w}'$ by showing $(\vec{w}-\vec{w}')\cdot(\vec{w}-\vec{w}') = 0$. Since 
  $$ (\vec{w}-\vec{w}')\cdot(\vec{w}-\vec{w}') = \vec{w}\cdot(\vec{w}-\vec{w}')-\vec{w}'\cdot(\vec{w}-\vec{w}'), $$ 
  we show that $\vec{w}\cdot(\vec{w}-\vec{w}') = 0$ and note that the same reasoning applies to $\vec{w}'\cdot(\vec{w}-\vec{w}')$. 
  We see that
  $$ \vec{w} - \vec{w}' = (\vec{w} - \vec{x}) + (\vec{x} - \vec{w}'). $$ 
  Since $\vec{w}-\vec{x} = -(\vec{x}-\vec{w})$ is in $V^{\perp}$, and $\vec{x}-\vec{w}'$ is also in $V^{\perp}$ by
  hypothesis, it must be that $\vec{w}-\vec{w}'$ in $V^{\perp}$. Since $w \in V$, it must be the case that 
  $$ \vec{w} \cdot (\vec{w} - \vec{w}') = 0. $$

</div>

</section>

<section class="primary">

# Linearity

The transformation $T(\vec{x}) = \text{proj}_V(\vec{x}) = \vec{x}^{\parallel}$ from $\bbr^n$ to $\bbr^n$ is linear.
Geometrically speaking, this is fairly obvious. We expect two vectors appended end to end would yield a projection 
corresponding to the sum of the projections of the vectors. Similarly, we would expect scaling a vector $\vec{x}$ 
by $k$ would yield a projection of length $k\cdot\text{proj}_v(\vec{x})$.

<div class="proof">

Consider transformation $T(\vec{x}) = \text{proj}_V(\vec{x}) = \vec{x}^{\parallel}$ from $\bbr^n$ to $\bbr^n$.
Let $\vec{x}, \;\vec{y} \in \bbr^n$ and $k$ be a scalar, and consider the following:

1. $T(\vec{x}+\vec{y}) = \text{proj}_V(\vec{x}+\vec{y}) = (\vec{x}+\vec{y})^{\parallel}
                       = (\vec{x}+\vec{y}) - (\vec{x}+\vec{y})^{\perp}
                       = \vec{x} - \vec{x}^{\perp} + \vec{y} - \vec{y}^{\perp}
                       = T(\vec{x}) + T(\vec{y})$.<br />

2. $T(k\vec{x}) = \text{proj}_V(k\vec{x}) = (k\vec{x})^{\parallel} = k\vec{x}-(k\vec{x})^{\perp} 
                = k(\vec{x}-\vec{x}^{\perp}) = k\text{proj}_V(\vec{x})$.

Therefore the transformation $T(\vec{x})$ is linear.

</div>

</section>

<section class="primary">

# Length

Consider a subspace $V$ of $\bbr^n$ and a vector $\vec{x}$ in $\bbr^n$. Then
$$ \norm{\text{proj}_V\vec{x}} \leq \norm{\vec{x}}. $$ 
The statement is an equality if and only if $\vec{x}$ is in $V$.

<div class="proof">

By the [Pythagorean theorem](pythagorean_theorem), 
$$ \norm{\vec{x}}^2 = \norm{\text{proj}_V\vec{x}}^2 + \norm{\vec{x}^{\perp}}^2. $$
Thus $\norm{\text{proj}_V\vec{x}} \leq \norm{\vec{x}}$ as claimed.

</div>

<section class="secondary">

## Closest Vector

An alternative classification comes from the observation that $\text{proj}_V(\vec{x})$ is the vector in $V$ closest to 
$\vec{x}$, in that $$ \norm{\vec{x} - \text{proj}_V(\vec{x})} < \norm{\vec{x}-\vec{v}}, $$ for all $\vec{v}$ in $V$ different 
from $\text{proj}_V(\vec{x})$. This interpretation is crucial in [least-squares problems](least_squares). 

</section>

</section>

<section class="primary">

# Orthonormal Basis

<div class="block excerpt">

![Standard Basis](/img/diagrams/linear/standard-basis.png)

If $V$ is a subspace of $\bbr^n$ with an [orthonormal basis](bases#orthonormal-bases) $\inflate{\vec{u}}{m}$, then
$$ 
  \text{proj}_V(\vec{x}) = \vec{x}^{\parallel} = (\vec{u}_1\cdot\vec{x})\vec{u}_1 + \cdots +
  (\vec{u}_m\cdot\vec{x})\vec{u}_m\text{,}
$$ 
for all $\vec{x}$ in $\bbr^n$. Note this follows directly from the proof of [orthogonal decomposition](#definition).

In addition, we can now view orthogonal decompositions in higher powers of $\bbr$ in another geometric manner.
Given the above formula, we note a projection of a subspace $V$ of $\bbr^n$ is the sum of projections of a given 
$\vec{x}$ on each line spanned by the basis vectors $\inflate{\vec{u}}{m}$ of $V$. If we let $V = \bbr^n$, this 
sum must be $\vec{x}$.
Furthermore, it then stands to reason that given an orthonormal basis $\inflate{\vec{u}}{n}$ of $\bbr^n$,
$$ \vec{x} = (\vec{u}_1\cdot\vec{x})\vec{u}_1+\cdots+(\vec{u}_n\cdot\vec{x})\vec{u}_n\text{,} $$ 
for all $\vec{x}$ in $\bbr^n$.

</div>

<div class="proof">

Apply the [formula for orthogonal projections](#orthonormal-basis) to the subspace $V = \bbr^n$ of $\bbr^n$ with orthonormal 
basis $\inflate{\vec{u}}{n}$. Then $\text{proj}_V(\vec{x}) = \vec{x}$, for all $\vec{x}$ in $V$. Therefore, by decomposition,
$$ \vec{x} = (\vec{u}_1\cdot\vec{x})\vec{u}_1+\cdots+(\vec{u}_n\cdot\vec{x})\vec{u}_n, $$
for all $\vec{x}$ in $\bbr^n$.

</div>

<section class="secondary">

## Inner Product Spaces

This concept can be extended to the general case of [inner product spaces](inner_product_spaces) as follows:

If $\inflate{g}{m}$ is an orthonormal basis of a subspace $W$ of an inner product space $V$, then
$$ \text{proj}_W\;f = \iprod{g_1}{\;f}g_1 + \cdots + \iprod{g_m}{\;f}g_m\text{,} $$
for all $\;f$ in $V$.

</section>

</section>

<section class="primary">

# Matrix Representation

Consider a subspace $V$ of $\bbr^n$ with orthonormal basis $\inflate{\vec{u}}{m}$. The matrix of the orthogonal
projection onto $V$ is
$$
  QQ^T,\quad\text{where }Q = \begin{bmatrix}
    \vert & \vert & & \vert \\
    \vec{u}_1 & \vec{u}_2 & \cdots & \vec{u}_m \\
    \vert & \vert & & \vert
  \end{bmatrix}\text{.}
$$

<div class="proof">

First consider the orthogonal projection
$$ \text{proj}_L(\vec{x}) = (\vec{u}_1 \cdot \vec{x})\vec{u}_1 $$
onto a line $L$ in $\bbr^n$, where $\vec{u}_1$ is a unit vector in $L$. If we view the vector $\vec{u}_1$ as an 
$n \times 1$ matrix and the scalar $\vec{u}_1\cdot\vec{x}$ as a $1 \times 1$ matrix, we can write
$$\begin{align*}
  \text{proj}_L(\vec{x}) &= \vec{u}_1(\vec{u}_1\cdot\vec{x}) \\
                         &= \vec{u}_1\vec{u}_1^T\vec{x}      \\
                         &= M\vec{x},
\end{align*}$$
where $M = \vec{u}_1\vec{u}_1^T$. Note that $\vec{u}_1$ is an $n \times 1$ matrix and $\vec{u}_1^T$ is $1 \times n$,
so that $M$ is $n \times n$, as expected. More generally, consider 
$$ \text{proj}_V(\vec{x}) = (\vec{u}_1\cdot\vec{x})\vec{u}_1 + \cdots + (\vec{u}_m\cdot\vec{x})\vec{u}_m $$
onto a subspace $V$ of $\bbr^n$ with orthonormal basis $\inflate{\vec{u}}{m}$. We can write
$$\begin{align*}
  \text{proj}_V(\vec{x}) &= \vec{u}_1\vec{u}_1^T\vec{x} + \cdots + \vec{u}_m\vec{u}_m^T\vec{x}                                 \\
                         &= (\vec{u}_1\vec{u}_1^T + \cdots + \vec{u}_m\vec{u}_m^T)\vec{x}                                      \\
                         &= \begin{bmatrix} \vert & & \vert \\ \vec{u}_1 & \cdots & \vec{u}_m \\ \vert & & \vert \end{bmatrix}
                            \begin{bmatrix} - & \vec{u}_1^T & - \\ & \vdots & \\ - & \vec{u}_m^T & - \end{bmatrix}\vec{x}.
\end{align*}$$

</div>

</section>


<section class="primary">

# See Also

- [Orthogonal Complement](orthogonal_complement)
- [Transpositions](transpositions)

</section>

<section class="primary">

# Notes

- Bretscher, Otto (2008). Linear Algebra with Applications (4th ed.). Pearson. ISBN 978-0-13-600926-9.

</section>
