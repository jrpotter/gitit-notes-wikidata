---
title: Covariance
categories: Statistics
icon: /img/icons/random-variables.png
...

<section class="primary">

# Summary

<div class="block">

![](/img/diagrams/statistics/covariance.png)

Let $X$ and $Y$ be two random variables. The **covariance** between $X$ and $Y$, denoted by $\text{Cov}(X, Y)$, is defined by
$$ \text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]. $$
Alternatively, we say that $$ \text{Cov}(X, Y) = E[XY] - E[X]E[Y]. $$
Similar to how [variance](variance) relays how much a single variable could change, covariance relays information on how
much two random variables change with respect to one another.

</div>

<div class="proof">

Expanding the right side of the original definition above we get
$$\begin{align*}
  \text{Cov}(X, Y) &= E[(X - E[X])(Y - E[Y])]                \\
                   &= E[XY - E[X]Y - XE[Y] + E[Y]E[X]]       \\
                   &= E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] \\
                   &= E[XY] - E[X]E[Y].
\end{align*}$$

</div>

</section>

<section class="primary">

# Independent Variables

If $X$ and $Y$ are random variables independent from one another, then $$ \text{Cov}(X, Y) = 0. $$

<div class="proof">

By the secondary expansion listed [above](#summary), we see that
$$ \text{Cov}(X, Y) = E[XY] - E[X]E[Y]. $$
But, by the [expectation of independent variables](expected_value#independent-variables), we see that
$E[XY] = E[X]E[Y]$. Our result follows.

</div>

</section>

<section class="primary">

# Properties

1. *Commutativity:* $$ \text{Cov}(X, Y) = \text{Cov}(Y, X). $$
2. *Variance:* $$ \text{Cov}(X, X) = \text{Var}(X). $$
3. *Linearity:* $$ \text{Cov}(aX, Y) = a\text{Cov}(X, Y). $$
4. *Summation:* $$\text{Cov}\left(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\right) = \sum_{i=1}^n\sum_{j=1}^m \text{Cov}(X_i, Y_j). $$

<div class="proof">

- Property 1:
:    Note $\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[(Y - E[Y])(X - E[X])]$. The proof follows.

- Property 2:
:    Note $\text{Cov}(X, Y) = E[(X - E[X])(X - E[X])] = E[(X - E[X])^2]$. The proof follows.

- Property 3:
:    Note that
     $$\begin{align*}
       \text{Cov}(aX, Y) &= E[(aX - E[aX])(Y - E[Y])] \\
                         &= E[(aX - aE[X])(Y - E[Y])] \\
                         &= aE[(X - E[X])(Y - E[Y])]  \\
                         &= a\text{Cov}(X, Y).
     \end{align*}$$

- Property 4:
:    Let $\mu_i = E[X_i]$ and $v_j = E[Y_j]$. Then
     $$ E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \mu_i, \quad E\left[\sum_{j=1}^m Y_j\right] = \sum_{j=1}^m v_j $$
     and
     $$\begin{align*}
       \text{Cov}\left(\sum_{i=1}^n X_i, \sum_{j=1}^m Y_j\right) 
         &= E\left[\left(\sum_{i=1}^n X_i - \sum_{i=1}^n \mu_i\right)\left(\sum_{j=1}^m Y_j - \sum_{j=1}^m v_j\right)\right] \\
         &= E\left[\sum_{i=1}^n (X_i - \mu_i)\sum_{j=1}^m(Y_j - v_j)\right]                                                  \\
         &= E\left[\sum_{i=1}^n\sum_{j=1}^m(X_i - \mu_i)(Y_j - v_j)\right]                                                   \\
         &= \sum_{i=1}^n\sum_{j=1}^m E[(X_i - \mu_i)(Y_j - v_j)].
     \end{align*}$$

</div>

</section>

<section class="primary">

# See Also

- [Expected Value](expected_value)
- [Variance](variance)

</section>

<section class="primary">

# Notes

- Ross, Sheldon (2010). A First Course in Probability (8th ed.). Pearson. ISBN 978-0-13-603313-4.

</section>