---
title: Bases
categories: Linear Algebra
icon: /img/icons/alternate-bases.png
...

<section class="primary">

# Summary

<div class="block excerpt">

![Sample Basis](/img/graphs/linear/sample-basis.png)

The vectors $\inflate{\vec{v}}{n}$ form a **basis** of subspace $V$ of $\bbr^n$ if they span $V$ and are linearly independent 
(and that the vectors are in $V$). Similarly, we say that elements $\inflate{\;f}{n}$ are a **basis** of $V$ if they span $V$ 
and are linearly independent. This means that every $\;f$ in $V$ can be written uniquely as a linear combination 
$\;f = c_1f_1 + \cdots + c_nf_n$.

Moreover, we see that vectors $\inflate{\vec{v}}{n}$ form a basis of $V$ if and only if every vector $\vec{v} \in V$ can be 
expressed uniquely as a linear combination $\vec{v} = c_1\vec{v}_1 + \cdots + c_m\vec{v}_m$.

</div>

<div class="proof">

$\forward$ There exists at least one solution since $\inflate{\vec{v}}{m}$ span $V$. Suppose 
$$ \vec{v} = c_1\vec{v}_1 + \cdots + c_m\vec{v}_m = d_1\vec{v}_1 + \cdots + d_m\vec{v}_m\text{.} $$ 
Then $$ \vec{0} = (c_1-d_1)\vec{v}_1 + \cdots + (c_m-d_m)\vec{v}_m\text{.} $$ 
Since the vectors are linearly independent, this must be the trivial relation, implying 
$$ c_1 = d_1, c_2 = d_2, \ldots, c_m = d_m. $$

$\backward$ Suppose $$ \vec{v} = c_1\vec{v}_1 + \cdots + c_m\vec{v}_m $$ is the unique representation of $\vec{v}$. 
Clearly the vectors span $V$ since every $\vec{v} \in V$ can be written as a linear combination of
$\inflate{\vec{v}}{m}$. Then this implies $$ \vec{0} = c_1\vec{v}_1 + \cdots + c_m\vec{v}_m $$ has a unique
representation. Therefore only the trivial relation exists, and the vectors are linearly independent, satisfying
the definition of a basis.

</div>

</section>

<section class="primary">

# Alternative Characterization

If the above proves difficult to show, we also note vectors $\inflate{\vec{v}}{n} \in \bbr^n$ form a basis of $\bbr^n$ 
if and only if the matrix
$$\begin{bmatrix}
  \vert     &        & \vert     \\
  \vec{v}_1 & \cdots & \vec{v}_n \\
  \vert     &        & \vert
\end{bmatrix}$$
is invertible.

<div class="proof">

Consider vectors $\inflate{\vec{v}}{n}$. By linear combinations, these form a basis of $\bbr^n$ 
if and only if every vector $\vec{b}$ in $\bbr^n$ can be written uniquely as a linear combination of the vectors
$\inflate{\vec{v}}{n}$:
$$
  \vec{b} = c_1\vec{v}_1 + \cdots + c_n\vec{v}_n =
  \begin{bmatrix} \vert &  & \vert \\ \vec{v}_1 & \cdots & \vec{v}_n \\ \vert &  & \vert \end{bmatrix}
  \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix}
$$
By definition of invertibility, the linear system
$$
  \begin{bmatrix} \vert &  & \vert \\ \vec{v}_1 & \cdots & \vec{v}_n \\ \vert &  & \vert \end{bmatrix}
  \begin{bmatrix} c_1 \\ \vdots \\ c_n \end{bmatrix} = \vec{b}
$$
has a unique solution for all $\vec{b}$ if and only if the $n \times n$ matrix
$$ \begin{bmatrix} \vert &  & \vert \\ \vec{v}_1 & \cdots & \vec{v}_n \\ \vert &  & \vert \end{bmatrix} $$
is invertible.

</div>

</section>

<section class="primary">

# Equivalence

Of foremost importance is the fact that though there are potentially many 
bases of a particular subspace, *all* bases of a (linear subspace) have the same number of elements.

- All bases of subspace $V$ of $\bbr^n$ consist of the same number of vectors.<br />  
  <div class="proof">
  The proof stems from the fact that if vectors $\inflate{\vec{v}}{p}$ and $\inflate{\vec{w}}{q}$ in a subspace $V$ of 
  $\bbr^n$ are defined in such a way that the vectors $\inflate{\vec{v}}{p}$ are linearly independent and vectors 
  $\inflate{\vec{w}}{q}$ span $V$, then $q \geq p$. Consider matrices 
  $$ 
    A = \begin{bmatrix} \vec{w}_1 & \cdots & \vec{w}_q \end{bmatrix} \text{ and }
    B = \begin{bmatrix} \vec{v}_1 & \cdots & \vec{v}_p \end{bmatrix} \text{.}
  $$ 
  Note that $\text{im}\;A = V$ since the vectors $\inflate{\vec{w}}{q}$ span $V$. The vectors $\inflate{\vec{v}}{p}$ are 
  in this image and so 
  $$ \vec{v}_1 = A\vec{u}_1, \;\vec{v}_2 = A\vec{u}_2, \;\ldots, \;\vec{v}_p = A\vec{u}_p $$
  for some vectors $\inflate{\vec{u}}{p}$ in $\bbr^q$ (note $A$ is a $q \times n$ matrix). Thus 
  $$
    B = \begin{bmatrix} \vec{v}_1 & \cdots & \vec{v}_p \end{bmatrix} = 
    A \begin{bmatrix} \vec{u}_1 & \cdots & \vec{u}_p \end{bmatrix} \text{, or }
    B = AC,
  $$
  where $$ C = \begin{bmatrix} \vec{u}_1 & \cdots & \vec{u}_p \end{bmatrix}. $$
  The kernel of $C$ is a subset of the kernel of $B$ since $C\vec{x} = \vec{0} \Rightarrow A(C\vec{x}) =
  A\vec{0} = \vec{0}$. But the kernel of $B$ is $\{\vec{0}\}$ since $\inflate{\vec{v}}{p}$ are linearly independent. 
  Thus, $\text{ker}(C) = \{\vec{0}\}$ as well meaning $C$ has at least as many rows as columns[^1], that is, $q \geq p$.<br />  
  Therefore, given two bases $\inflate{\vec{v}}{p}$ and $\inflate{\vec{w}}{q}$ of $V$, $p \geq q$ and $q \geq p$ so $p = q$.
  </div><br />

- Similarly, if a linear space $V$ has a basis with $n$ elements, then all other bases of $V$ consist of $n$
  elements as well.<br />  
  <div class="proof">  
  Consider two bases $\alpha = (\inflate{\;f}{n})$ and $\beta = (\inflate{g}{m})$ of $V$; we must show that $n = m$. 
  We will first show that the $m$ vectors $\bcoor{g_1} \ldots \bcoor{g_m}$ in $\bbr^n$ are linearly independent, which 
  implies that $m \leq n$ by the [maximum number of linearly independent vectors](subspaces#linear-independence) possible. 
  Consider a relation 
  $$ c_1\bcoor{g_1} + \cdots + c_m\bcoor{g_m} = \vec{0}\text{.} $$ 
  By [linearity of coordinates](coordinates#linearity), we have 
  $$ \bcoor{c_1g_1+\cdots+c_mg_m} = \vec{0},\text{ so that } c_1g_1 + \cdots + c_mg_m = 0\text{.} $$ 
  Since the elements $\inflate{g}{m}$ are linearly independent, it follows that $c_1 = \cdots = c_m = 0$, meaning that
  $c_1\bcoor{g_1}+\cdots+c_m\bcoor{g_m}=\vec{0}$ is the trivial relation, as claimed. We can apply the same for $\alpha$ and 
  see that $n \leq m$ and $m \leq n$ meaning $n = m$.
  </div>

</section>

<section class="primary">

# Orthonormal Bases

The vectors $\inflate{\vec{u}}{m}$ in $\bbr^n$ are called **orthonormal** if they are all unit vectors and 
orthogonal to one another:
$$
  \vec{u}_i \cdot \vec{u}_j = \begin{cases}
    1\text{ if }i = j \\
    0\text{ if }i \neq j
  \end{cases}
$$

We note orthonormal vectors are linearly independent and as such, orthonormal vectors $\inflate{\vec{u}}{n}$ in $\bbr^n$ 
form a basis of $\bbr^n$.

<div class="proof">

We note $n$ linearly independent vectors form a basis of $\bbr^n$ so all that remains is showing orthonormal vectors are
indeed linearly independent. Consider a relation 
$$ c_1\vec{u}_1 + c_2\vec{u}_2 + \cdots + c_i\vec{u}_i + \cdots + c_m\vec{u}_m = \vec{0} $$
among the orthonormal vectors $\inflate{\vec{u}}{m}$ in $\bbr^n$. Let us form the dot product of each side
of this equation with $\vec{u}_i$:
$$
  (c_1\vec{u}_1+c_2\vec{u}_2+\cdots+c_i\vec{u}_i+\cdots+c_m\vec{u}_m)\cdot\vec{u}_i = \vec{0}\cdot\vec{u}_i = 0
  \text{.}
$$
Because the dot product is distributive,
$$
  c_1(\vec{u}_1\cdot\vec{u}_i) + c_2(\vec{u}_2\cdot\vec{u}_i) + \cdots + c_i(\vec{u}_i\cdot\vec{u}_i) +
  \cdots+c_m(\vec{u}_m\cdot\vec{u}_i) = 0\text{.}
$$
We know that $\vec{u}_i\cdot\vec{u}_i = 1$, and all other dot products are zero. Therefore $c_i = 0$.
Since this holds for all $i = 1, \ldots, m$, it follows that the vectors $\inflate{\vec{u}}{m}$ are
linearly independent.

</div>

</section>

<section class="primary">

# Eigenbases

Consider an $n \times n$ matrix $A$. A basis of $\bbr^n$ consisting of eigenvectors of $A$ is 
called an **eigenbasis** for $A$. A similar definition exists for linear spaces.

Note if there are $n$ total eigenvectors (the sum total of the number of eigenvectors in each eigenspace), then these 
eigenvectors form an eigenbasis for $A$. That is, there exists an eigenbasis for an $n \times n$ matrix $A$ if and only 
if the geometric multiplicities of the eigenvalues add up to $n$.

<div class="proof">

We will prove the above claim by showing that the eigenvectors $\inflate{\vec{v}}{s}$ obtained by concatenating
each basis of each eigenspace of $A$ are linearly independent. Then the proof follows by noting $n = s$ by hypothesis.

For the sake of contradiction, suppose the eigenvectors $\inflate{\vec{v}}{s}$ are linearly dependent. Let
$\vec{v}_m$ be the first redundant vector in this list, with
$$ \vec{v}_m = c_1\vec{v}_1 + \cdots + c_{m-1}\vec{v}_{m-1}. $$
Suppose that $A\vec{v}_i = \lambda_i\vec{v}_i$. Note there must be at least one nonzero coefficient $c_k$ such that
$\lambda_k \neq \lambda_m$ since $\vec{v}_m$ cannot be expressed as a linear combination of vectors $\vec{v}_i$
that are all in the same eigenspace $E_{\lambda_{m}}$ (since the basis of each eigenspace is linearly independent by definition). 
We see that
$$\begin{align*}
  \vec{v}_m &= c_1\vec{v}_1 + \cdots + c_{m-1}\vec{v}_{m-1} \Rightarrow                                       \\
  (A - \lambda_m I_n)\vec{v}_m &= (A - \lambda_m I_n)c_1\vec{v}_1 + \cdots
                                + (A - \lambda_m I_n)c_{m-1}\vec{v}_{m-1} \Rightarrow                         \\
  \vec{0} = (\lambda_m - \lambda_m)\vec{v}_m &= (\lambda_1 - \lambda_m)c_1\vec{v}_1 + \cdots 
                                              + (\lambda_k - \lambda_m)c_k\vec{v}_k + \cdots
                                              + (\lambda_{m-1} - \lambda_m)c_{m-1}\vec{v}_{m-1}.
\end{align*}$$
where $(\lambda_k - \lambda_m)c_k \neq 0$. Thus there is a nontrivial relation among vectors
$\inflate{\vec{v}}{m-1}$, contradicting our assumption that $\vec{v}_m$ is the first redundant vector in the list.

</div>

</section>

<section class="primary">

# See Also

- [Change of Basis](change_of_basis)
- [Coordinates](coordinates)

</section>

<section class="primary">

# Notes

- Bretscher, Otto (2008). Linear Algebra with Applications (4th ed.). Pearson. ISBN 978-0-13-600926-9.

</section>

<!-- Footnotes -->

[^1]: [Zero Dimension Kernel](subspaces#kernel)
